{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff6c0cf3",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> Module containing the reading and conversion logic from the sl2 and sl3 files to csv and shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20357c4",
   "metadata": {},
   "source": [
    "We need to do several processing steps to go from the `sl2` or `sl3` data to a `csv` and `shape` we can use in our GIS software.\n",
    "\n",
    "**From depth in meter to bottom height in mNAP**\n",
    "\n",
    "To convert from depth measurement in meter to bottom height measurement in mNAP we need to know the height in mNAP from which the depth measurements were taken. Currently we add this height manually in the filename in cmNAP. Some examples:\n",
    "\n",
    "- `2024-07-11_zuiderpark Hoogeveen2_+1075cmnap.sl2`\n",
    "- `Sonar_2022-04-26_21.07.11beschrijving+0765cmNAP.sl3`\n",
    "\n",
    "We extract this height from the filename.\n",
    "\n",
    "The height of the Sonar boot is also stored in the `gps_altitude` column from the `sl2` and `sl3` files, but we haven't yet implemented the conversion from this height to mNAP height.\n",
    "\n",
    "**Filter relevant facts**\n",
    "\n",
    "We only need the facts that have the value \"primary\" in the column \"survey\".\n",
    "\n",
    "**Transformation to the correct CRS**\n",
    "\n",
    "The co繹rdinates in the Sonar files are in crs WGS84 (epsg:4326), we need to convert those to the crs we use, which is RDN Amersfoort (epsg:28992).\n",
    "We accomplish that by using the `geopandas` method `.set_crs` and `.to_crs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b60596",
   "metadata": {
    "time_run": "3:50:29p"
   },
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11811fe",
   "metadata": {
    "time_run": "3:50:29p"
   },
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f0b9fd",
   "metadata": {},
   "source": [
    "## Importing modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95fd299",
   "metadata": {},
   "source": [
    "We will use [sonarlight](https://github.com/KennethTM/sonarlight) to read the measurements from the `sl2` or `sl3` files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573f7eea",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "Previously we used [sslib](https://github.com/opensounder/python-sllib) to parse the sonar files. But this package latest commit was 4 years ago. This `sslib` package also has less stars and the `sonarlight` has some neat extra features, such as simple conversion to a Pandas dataframe.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43aeab72",
   "metadata": {
    "time_run": "3:50:29p"
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "from sonarlight import Sonar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50b8607",
   "metadata": {},
   "source": [
    "## Load sl3 file for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fa2b1e",
   "metadata": {
    "time_run": "3:50:29p"
   },
   "outputs": [],
   "source": [
    "sl3_f = Path(\"../test/Sonar_2022-04-26_21.07.11beschrijving+0765cmNAP.sl3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1360bb29",
   "metadata": {
    "time_run": "3:50:29p"
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def read_sl(\r\n",
    "    filepath: Path # The absolute location of the file to convert\r\n",
    "    )->Sonar:\r\n",
    "    return Sonar(str(filepath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a27dd6f",
   "metadata": {
    "time_run": "3:50:29p"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Summary of SL3 file:\n",
       "\n",
       "- Primary channel with 1320 frames\n",
       "- Secondary channel with 1320 frames\n",
       "- Downscan channel with 1320 frames\n",
       "- Sidescan channel with 1319 frames\n",
       "\n",
       "Start time: 2022-04-26 11:08:49.101999998\n",
       "End time: 2022-04-26 11:10:19.315000057\n",
       "\n",
       "File info: version 3, device 2, blocksize 3200, frame version 10"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sl3_d = read_sl(sl3_f)\r\n",
    "sl3_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6ff2ba",
   "metadata": {
    "time_run": "3:50:29p"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sonarlight.sonar_class.Sonar"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sl3_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0589d9f",
   "metadata": {
    "time_run": "3:50:29p"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>survey</th>\n",
       "      <th>datetime</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>min_range</th>\n",
       "      <th>max_range</th>\n",
       "      <th>water_depth</th>\n",
       "      <th>gps_speed</th>\n",
       "      <th>gps_heading</th>\n",
       "      <th>gps_altitude</th>\n",
       "      <th>bottom_index</th>\n",
       "      <th>frames</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1570</th>\n",
       "      <td>174</td>\n",
       "      <td>primary</td>\n",
       "      <td>2022-04-26 11:08:49.101999998</td>\n",
       "      <td>674208</td>\n",
       "      <td>6913399</td>\n",
       "      <td>6.076888</td>\n",
       "      <td>52.748741</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>36.576000</td>\n",
       "      <td>0.606861</td>\n",
       "      <td>0.127106</td>\n",
       "      <td>0.246756</td>\n",
       "      <td>-1.91</td>\n",
       "      <td>50</td>\n",
       "      <td>[137, 137, 137, 137, 137, 129, 124, 119, 114, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1573</th>\n",
       "      <td>174</td>\n",
       "      <td>secondary</td>\n",
       "      <td>2022-04-26 11:08:49.101999998</td>\n",
       "      <td>674208</td>\n",
       "      <td>6913399</td>\n",
       "      <td>6.076888</td>\n",
       "      <td>52.748741</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>36.576000</td>\n",
       "      <td>0.606861</td>\n",
       "      <td>0.127106</td>\n",
       "      <td>0.246756</td>\n",
       "      <td>-1.91</td>\n",
       "      <td>50</td>\n",
       "      <td>[137, 137, 137, 137, 137, 129, 124, 119, 114, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1576</th>\n",
       "      <td>174</td>\n",
       "      <td>downscan</td>\n",
       "      <td>2022-04-26 11:08:49.239000082</td>\n",
       "      <td>674207</td>\n",
       "      <td>6913400</td>\n",
       "      <td>6.076879</td>\n",
       "      <td>52.748746</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.945601</td>\n",
       "      <td>0.606861</td>\n",
       "      <td>0.127106</td>\n",
       "      <td>0.246756</td>\n",
       "      <td>-1.91</td>\n",
       "      <td>38</td>\n",
       "      <td>[152, 152, 152, 152, 129, 143, 140, 137, 140, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1577</th>\n",
       "      <td>352</td>\n",
       "      <td>sidescan</td>\n",
       "      <td>2022-04-26 11:08:49.240000010</td>\n",
       "      <td>674207</td>\n",
       "      <td>6913400</td>\n",
       "      <td>6.076879</td>\n",
       "      <td>52.748746</td>\n",
       "      <td>-39.989758</td>\n",
       "      <td>39.989758</td>\n",
       "      <td>0.606861</td>\n",
       "      <td>0.127106</td>\n",
       "      <td>0.246756</td>\n",
       "      <td>-1.91</td>\n",
       "      <td>21</td>\n",
       "      <td>[42, 26, 41, 38, 43, 46, 43, 47, 50, 50, 53, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1579</th>\n",
       "      <td>175</td>\n",
       "      <td>primary</td>\n",
       "      <td>2022-04-26 11:08:49.249000072</td>\n",
       "      <td>674208</td>\n",
       "      <td>6913399</td>\n",
       "      <td>6.076888</td>\n",
       "      <td>52.748741</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.992880</td>\n",
       "      <td>0.609836</td>\n",
       "      <td>0.122237</td>\n",
       "      <td>0.247439</td>\n",
       "      <td>-1.91</td>\n",
       "      <td>469</td>\n",
       "      <td>[216, 216, 216, 216, 216, 216, 216, 216, 216, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id     survey                      datetime       x        y  \\\n",
       "1570  174    primary 2022-04-26 11:08:49.101999998  674208  6913399   \n",
       "1573  174  secondary 2022-04-26 11:08:49.101999998  674208  6913399   \n",
       "1576  174   downscan 2022-04-26 11:08:49.239000082  674207  6913400   \n",
       "1577  352   sidescan 2022-04-26 11:08:49.240000010  674207  6913400   \n",
       "1579  175    primary 2022-04-26 11:08:49.249000072  674208  6913399   \n",
       "\n",
       "      longitude   latitude  min_range  max_range  water_depth  gps_speed  \\\n",
       "1570   6.076888  52.748741   0.000000  36.576000     0.606861   0.127106   \n",
       "1573   6.076888  52.748741   0.000000  36.576000     0.606861   0.127106   \n",
       "1576   6.076879  52.748746   0.000000  21.945601     0.606861   0.127106   \n",
       "1577   6.076879  52.748746 -39.989758  39.989758     0.606861   0.127106   \n",
       "1579   6.076888  52.748741   0.000000   3.992880     0.609836   0.122237   \n",
       "\n",
       "      gps_heading  gps_altitude  bottom_index  \\\n",
       "1570     0.246756         -1.91            50   \n",
       "1573     0.246756         -1.91            50   \n",
       "1576     0.246756         -1.91            38   \n",
       "1577     0.246756         -1.91            21   \n",
       "1579     0.247439         -1.91           469   \n",
       "\n",
       "                                                 frames  \n",
       "1570  [137, 137, 137, 137, 137, 129, 124, 119, 114, ...  \n",
       "1573  [137, 137, 137, 137, 137, 129, 124, 119, 114, ...  \n",
       "1576  [152, 152, 152, 152, 129, 143, 140, 137, 140, ...  \n",
       "1577  [42, 26, 41, 38, 43, 46, 43, 47, 50, 50, 53, 5...  \n",
       "1579  [216, 216, 216, 216, 216, 216, 216, 216, 216, ...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sl3_d.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a809fc",
   "metadata": {
    "time_run": "3:50:29p"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>survey</th>\n",
       "      <th>datetime</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>min_range</th>\n",
       "      <th>max_range</th>\n",
       "      <th>water_depth</th>\n",
       "      <th>gps_speed</th>\n",
       "      <th>gps_heading</th>\n",
       "      <th>gps_altitude</th>\n",
       "      <th>bottom_index</th>\n",
       "      <th>frames</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1570</th>\n",
       "      <td>174</td>\n",
       "      <td>primary</td>\n",
       "      <td>2022-04-26 11:08:49.101999998</td>\n",
       "      <td>674208</td>\n",
       "      <td>6913399</td>\n",
       "      <td>6.076888</td>\n",
       "      <td>52.748741</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>36.576000</td>\n",
       "      <td>0.606861</td>\n",
       "      <td>0.127106</td>\n",
       "      <td>0.246756</td>\n",
       "      <td>-1.91</td>\n",
       "      <td>50</td>\n",
       "      <td>[137, 137, 137, 137, 137, 129, 124, 119, 114, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1573</th>\n",
       "      <td>174</td>\n",
       "      <td>secondary</td>\n",
       "      <td>2022-04-26 11:08:49.101999998</td>\n",
       "      <td>674208</td>\n",
       "      <td>6913399</td>\n",
       "      <td>6.076888</td>\n",
       "      <td>52.748741</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>36.576000</td>\n",
       "      <td>0.606861</td>\n",
       "      <td>0.127106</td>\n",
       "      <td>0.246756</td>\n",
       "      <td>-1.91</td>\n",
       "      <td>50</td>\n",
       "      <td>[137, 137, 137, 137, 137, 129, 124, 119, 114, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1576</th>\n",
       "      <td>174</td>\n",
       "      <td>downscan</td>\n",
       "      <td>2022-04-26 11:08:49.239000082</td>\n",
       "      <td>674207</td>\n",
       "      <td>6913400</td>\n",
       "      <td>6.076879</td>\n",
       "      <td>52.748746</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.945601</td>\n",
       "      <td>0.606861</td>\n",
       "      <td>0.127106</td>\n",
       "      <td>0.246756</td>\n",
       "      <td>-1.91</td>\n",
       "      <td>38</td>\n",
       "      <td>[152, 152, 152, 152, 129, 143, 140, 137, 140, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1577</th>\n",
       "      <td>352</td>\n",
       "      <td>sidescan</td>\n",
       "      <td>2022-04-26 11:08:49.240000010</td>\n",
       "      <td>674207</td>\n",
       "      <td>6913400</td>\n",
       "      <td>6.076879</td>\n",
       "      <td>52.748746</td>\n",
       "      <td>-39.989758</td>\n",
       "      <td>39.989758</td>\n",
       "      <td>0.606861</td>\n",
       "      <td>0.127106</td>\n",
       "      <td>0.246756</td>\n",
       "      <td>-1.91</td>\n",
       "      <td>21</td>\n",
       "      <td>[42, 26, 41, 38, 43, 46, 43, 47, 50, 50, 53, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1579</th>\n",
       "      <td>175</td>\n",
       "      <td>primary</td>\n",
       "      <td>2022-04-26 11:08:49.249000072</td>\n",
       "      <td>674208</td>\n",
       "      <td>6913399</td>\n",
       "      <td>6.076888</td>\n",
       "      <td>52.748741</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.992880</td>\n",
       "      <td>0.609836</td>\n",
       "      <td>0.122237</td>\n",
       "      <td>0.247439</td>\n",
       "      <td>-1.91</td>\n",
       "      <td>469</td>\n",
       "      <td>[216, 216, 216, 216, 216, 216, 216, 216, 216, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1582</th>\n",
       "      <td>175</td>\n",
       "      <td>secondary</td>\n",
       "      <td>2022-04-26 11:08:49.249000072</td>\n",
       "      <td>674208</td>\n",
       "      <td>6913399</td>\n",
       "      <td>6.076888</td>\n",
       "      <td>52.748741</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.992880</td>\n",
       "      <td>0.609836</td>\n",
       "      <td>0.122237</td>\n",
       "      <td>0.247439</td>\n",
       "      <td>-1.91</td>\n",
       "      <td>469</td>\n",
       "      <td>[216, 216, 216, 216, 216, 216, 216, 216, 216, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1585</th>\n",
       "      <td>175</td>\n",
       "      <td>downscan</td>\n",
       "      <td>2022-04-26 11:08:49.309000015</td>\n",
       "      <td>674207</td>\n",
       "      <td>6913400</td>\n",
       "      <td>6.076879</td>\n",
       "      <td>52.748746</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.407920</td>\n",
       "      <td>0.609836</td>\n",
       "      <td>0.122237</td>\n",
       "      <td>0.247439</td>\n",
       "      <td>-1.91</td>\n",
       "      <td>354</td>\n",
       "      <td>[152, 152, 152, 152, 152, 152, 152, 152, 152, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1586</th>\n",
       "      <td>354</td>\n",
       "      <td>sidescan</td>\n",
       "      <td>2022-04-26 11:08:49.311000109</td>\n",
       "      <td>674207</td>\n",
       "      <td>6913400</td>\n",
       "      <td>6.076879</td>\n",
       "      <td>52.748746</td>\n",
       "      <td>-2.987040</td>\n",
       "      <td>2.987040</td>\n",
       "      <td>0.609836</td>\n",
       "      <td>0.122237</td>\n",
       "      <td>0.247439</td>\n",
       "      <td>-1.91</td>\n",
       "      <td>285</td>\n",
       "      <td>[19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1587</th>\n",
       "      <td>176</td>\n",
       "      <td>primary</td>\n",
       "      <td>2022-04-26 11:08:49.290999889</td>\n",
       "      <td>674208</td>\n",
       "      <td>6913399</td>\n",
       "      <td>6.076888</td>\n",
       "      <td>52.748741</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.992880</td>\n",
       "      <td>0.609836</td>\n",
       "      <td>0.118954</td>\n",
       "      <td>0.248092</td>\n",
       "      <td>-1.91</td>\n",
       "      <td>469</td>\n",
       "      <td>[216, 216, 216, 216, 216, 216, 216, 216, 216, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1590</th>\n",
       "      <td>176</td>\n",
       "      <td>secondary</td>\n",
       "      <td>2022-04-26 11:08:49.290999889</td>\n",
       "      <td>674208</td>\n",
       "      <td>6913399</td>\n",
       "      <td>6.076888</td>\n",
       "      <td>52.748741</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.992880</td>\n",
       "      <td>0.609836</td>\n",
       "      <td>0.118954</td>\n",
       "      <td>0.248092</td>\n",
       "      <td>-1.91</td>\n",
       "      <td>469</td>\n",
       "      <td>[216, 216, 216, 216, 216, 216, 216, 216, 216, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1593</th>\n",
       "      <td>176</td>\n",
       "      <td>downscan</td>\n",
       "      <td>2022-04-26 11:08:49.315000057</td>\n",
       "      <td>674207</td>\n",
       "      <td>6913400</td>\n",
       "      <td>6.076879</td>\n",
       "      <td>52.748746</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.407920</td>\n",
       "      <td>0.609836</td>\n",
       "      <td>0.118954</td>\n",
       "      <td>0.248092</td>\n",
       "      <td>-1.91</td>\n",
       "      <td>354</td>\n",
       "      <td>[228, 228, 228, 228, 228, 228, 228, 228, 228, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594</th>\n",
       "      <td>355</td>\n",
       "      <td>sidescan</td>\n",
       "      <td>2022-04-26 11:08:49.316999912</td>\n",
       "      <td>674207</td>\n",
       "      <td>6913400</td>\n",
       "      <td>6.076879</td>\n",
       "      <td>52.748746</td>\n",
       "      <td>-2.987040</td>\n",
       "      <td>2.987040</td>\n",
       "      <td>0.609836</td>\n",
       "      <td>0.118954</td>\n",
       "      <td>0.248092</td>\n",
       "      <td>-1.91</td>\n",
       "      <td>285</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>177</td>\n",
       "      <td>primary</td>\n",
       "      <td>2022-04-26 11:08:49.378000021</td>\n",
       "      <td>674208</td>\n",
       "      <td>6913399</td>\n",
       "      <td>6.076888</td>\n",
       "      <td>52.748741</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.992880</td>\n",
       "      <td>0.612811</td>\n",
       "      <td>0.114456</td>\n",
       "      <td>0.248715</td>\n",
       "      <td>-1.91</td>\n",
       "      <td>471</td>\n",
       "      <td>[216, 216, 216, 216, 216, 216, 216, 216, 216, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599</th>\n",
       "      <td>177</td>\n",
       "      <td>secondary</td>\n",
       "      <td>2022-04-26 11:08:49.378000021</td>\n",
       "      <td>674208</td>\n",
       "      <td>6913399</td>\n",
       "      <td>6.076888</td>\n",
       "      <td>52.748741</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.992880</td>\n",
       "      <td>0.612811</td>\n",
       "      <td>0.114456</td>\n",
       "      <td>0.248715</td>\n",
       "      <td>-1.91</td>\n",
       "      <td>471</td>\n",
       "      <td>[216, 216, 216, 216, 216, 216, 216, 216, 216, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1602</th>\n",
       "      <td>177</td>\n",
       "      <td>downscan</td>\n",
       "      <td>2022-04-26 11:08:49.404000044</td>\n",
       "      <td>674207</td>\n",
       "      <td>6913400</td>\n",
       "      <td>6.076879</td>\n",
       "      <td>52.748746</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.407920</td>\n",
       "      <td>0.612811</td>\n",
       "      <td>0.114456</td>\n",
       "      <td>0.248715</td>\n",
       "      <td>-1.91</td>\n",
       "      <td>356</td>\n",
       "      <td>[228, 228, 228, 228, 228, 228, 228, 228, 228, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1603</th>\n",
       "      <td>357</td>\n",
       "      <td>sidescan</td>\n",
       "      <td>2022-04-26 11:08:49.404999971</td>\n",
       "      <td>674207</td>\n",
       "      <td>6913400</td>\n",
       "      <td>6.076879</td>\n",
       "      <td>52.748746</td>\n",
       "      <td>-2.987040</td>\n",
       "      <td>2.987040</td>\n",
       "      <td>0.612811</td>\n",
       "      <td>0.114456</td>\n",
       "      <td>0.248715</td>\n",
       "      <td>-1.91</td>\n",
       "      <td>287</td>\n",
       "      <td>[0, 0, 3, 3, 3, 3, 1, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id     survey                      datetime       x        y  \\\n",
       "1570  174    primary 2022-04-26 11:08:49.101999998  674208  6913399   \n",
       "1573  174  secondary 2022-04-26 11:08:49.101999998  674208  6913399   \n",
       "1576  174   downscan 2022-04-26 11:08:49.239000082  674207  6913400   \n",
       "1577  352   sidescan 2022-04-26 11:08:49.240000010  674207  6913400   \n",
       "1579  175    primary 2022-04-26 11:08:49.249000072  674208  6913399   \n",
       "1582  175  secondary 2022-04-26 11:08:49.249000072  674208  6913399   \n",
       "1585  175   downscan 2022-04-26 11:08:49.309000015  674207  6913400   \n",
       "1586  354   sidescan 2022-04-26 11:08:49.311000109  674207  6913400   \n",
       "1587  176    primary 2022-04-26 11:08:49.290999889  674208  6913399   \n",
       "1590  176  secondary 2022-04-26 11:08:49.290999889  674208  6913399   \n",
       "1593  176   downscan 2022-04-26 11:08:49.315000057  674207  6913400   \n",
       "1594  355   sidescan 2022-04-26 11:08:49.316999912  674207  6913400   \n",
       "1596  177    primary 2022-04-26 11:08:49.378000021  674208  6913399   \n",
       "1599  177  secondary 2022-04-26 11:08:49.378000021  674208  6913399   \n",
       "1602  177   downscan 2022-04-26 11:08:49.404000044  674207  6913400   \n",
       "1603  357   sidescan 2022-04-26 11:08:49.404999971  674207  6913400   \n",
       "\n",
       "      longitude   latitude  min_range  max_range  water_depth  gps_speed  \\\n",
       "1570   6.076888  52.748741   0.000000  36.576000     0.606861   0.127106   \n",
       "1573   6.076888  52.748741   0.000000  36.576000     0.606861   0.127106   \n",
       "1576   6.076879  52.748746   0.000000  21.945601     0.606861   0.127106   \n",
       "1577   6.076879  52.748746 -39.989758  39.989758     0.606861   0.127106   \n",
       "1579   6.076888  52.748741   0.000000   3.992880     0.609836   0.122237   \n",
       "1582   6.076888  52.748741   0.000000   3.992880     0.609836   0.122237   \n",
       "1585   6.076879  52.748746   0.000000   2.407920     0.609836   0.122237   \n",
       "1586   6.076879  52.748746  -2.987040   2.987040     0.609836   0.122237   \n",
       "1587   6.076888  52.748741   0.000000   3.992880     0.609836   0.118954   \n",
       "1590   6.076888  52.748741   0.000000   3.992880     0.609836   0.118954   \n",
       "1593   6.076879  52.748746   0.000000   2.407920     0.609836   0.118954   \n",
       "1594   6.076879  52.748746  -2.987040   2.987040     0.609836   0.118954   \n",
       "1596   6.076888  52.748741   0.000000   3.992880     0.612811   0.114456   \n",
       "1599   6.076888  52.748741   0.000000   3.992880     0.612811   0.114456   \n",
       "1602   6.076879  52.748746   0.000000   2.407920     0.612811   0.114456   \n",
       "1603   6.076879  52.748746  -2.987040   2.987040     0.612811   0.114456   \n",
       "\n",
       "      gps_heading  gps_altitude  bottom_index  \\\n",
       "1570     0.246756         -1.91            50   \n",
       "1573     0.246756         -1.91            50   \n",
       "1576     0.246756         -1.91            38   \n",
       "1577     0.246756         -1.91            21   \n",
       "1579     0.247439         -1.91           469   \n",
       "1582     0.247439         -1.91           469   \n",
       "1585     0.247439         -1.91           354   \n",
       "1586     0.247439         -1.91           285   \n",
       "1587     0.248092         -1.91           469   \n",
       "1590     0.248092         -1.91           469   \n",
       "1593     0.248092         -1.91           354   \n",
       "1594     0.248092         -1.91           285   \n",
       "1596     0.248715         -1.91           471   \n",
       "1599     0.248715         -1.91           471   \n",
       "1602     0.248715         -1.91           356   \n",
       "1603     0.248715         -1.91           287   \n",
       "\n",
       "                                                 frames  \n",
       "1570  [137, 137, 137, 137, 137, 129, 124, 119, 114, ...  \n",
       "1573  [137, 137, 137, 137, 137, 129, 124, 119, 114, ...  \n",
       "1576  [152, 152, 152, 152, 129, 143, 140, 137, 140, ...  \n",
       "1577  [42, 26, 41, 38, 43, 46, 43, 47, 50, 50, 53, 5...  \n",
       "1579  [216, 216, 216, 216, 216, 216, 216, 216, 216, ...  \n",
       "1582  [216, 216, 216, 216, 216, 216, 216, 216, 216, ...  \n",
       "1585  [152, 152, 152, 152, 152, 152, 152, 152, 152, ...  \n",
       "1586  [19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 1...  \n",
       "1587  [216, 216, 216, 216, 216, 216, 216, 216, 216, ...  \n",
       "1590  [216, 216, 216, 216, 216, 216, 216, 216, 216, ...  \n",
       "1593  [228, 228, 228, 228, 228, 228, 228, 228, 228, ...  \n",
       "1594  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1596  [216, 216, 216, 216, 216, 216, 216, 216, 216, ...  \n",
       "1599  [216, 216, 216, 216, 216, 216, 216, 216, 216, ...  \n",
       "1602  [228, 228, 228, 228, 228, 228, 228, 228, 228, ...  \n",
       "1603  [0, 0, 3, 3, 3, 3, 1, 1, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sl3_df_sml = sl3_d.df[0:16]\n",
    "sl3_df_sml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9e1a7e",
   "metadata": {},
   "source": [
    "## Notes to use `gps_altitude` column from `sl3`-file instead of extrachting the height of the measurement instrument from the filename"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4bdaff",
   "metadata": {},
   "source": [
    "The `sl3_df_sml` dataframe also has a column `gps_altitude`. Can you explain what this probably is. Given that longitude and latitude are given in the WGS84 coordinate system?\n",
    "\n",
    "##### Reply<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "The `gps_altitude` column is likely the **altitude (elevation) above the WGS84 ellipsoid**, measured in meters. \n",
    "\n",
    "In the WGS84 coordinate system, GPS devices typically provide three coordinates:\n",
    "- Longitude and latitude (horizontal position)\n",
    "- Altitude/elevation (vertical position)\n",
    "\n",
    "This GPS altitude is usually the height above the WGS84 reference ellipsoid (a mathematical model of Earth's shape), not the height above sea level or above a local datum like NAP.\n",
    "\n",
    "Looking at your data, the `gps_altitude` values are around 12.19 meters. This would be the height of the GPS antenna (on your sonar boat) above the WGS84 ellipsoid at that location.\n",
    "\n",
    "Is this different from the height information you're extracting from the filename (like \"+1075cmnap\")? What do you think the relationship might be between these two height measurements?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6dd36c",
   "metadata": {},
   "source": [
    "Great. How can I transform this `gps_altitude` to mNAP as used in the RDnew Amersfoort system (epsg:28992)?\n",
    "\n",
    "##### Reply<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "However, there's an important distinction to make here:\n",
    "\n",
    "**GPS altitude and coordinate system transformations work differently than horizontal coordinates.**\n",
    "\n",
    "When you transform longitude/latitude from WGS84 (epsg:4326) to RD New (epsg:28992) using `.to_crs()`, it handles the horizontal position transformation well. But the vertical component (altitude) is trickier because:\n",
    "\n",
    "1. GPS altitude is height above the WGS84 ellipsoid\n",
    "2. NAP (Normaal Amsterdams Peil) is a height above mean sea level\n",
    "3. These two reference surfaces don't have a simple constant offset - the difference varies by location\n",
    "\n",
    "To properly convert GPS altitude to mNAP, you typically need a **geoid model** or a transformation grid specific to the Netherlands (like NLGEO2018 or similar). This accounts for the local variations between the ellipsoid and the geoid (mean sea level).\n",
    "\n",
    "Do you have access to such a transformation tool, or is there a reason you're interested in converting the GPS altitude? I notice you're already extracting the NAP height from the filename - is that serving a different purpose?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b762e1fa",
   "metadata": {},
   "source": [
    "It would be good if we could skip the NAP height from the filename. Since that is a manual proces. Which may give errors or be forgotten when the file is uploaded.\n",
    "\n",
    "##### Reply<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "That makes sense - automating this would definitely reduce errors and make the workflow more robust!\n",
    "\n",
    "To convert GPS altitude (WGS84 ellipsoid height) to mNAP, you'll need to apply a geoid correction. In the Netherlands, the standard approach is to use the **RDNAPTRANS** transformation, which includes both horizontal (RD) and vertical (NAP) transformations.\n",
    "\n",
    "Python libraries that can handle this include:\n",
    "- **pyproj** - can handle vertical transformations with the right geoid model\n",
    "- **PROJ** (the underlying library) - supports compound CRS transformations\n",
    "\n",
    "The key is to use a compound coordinate reference system that includes both horizontal and vertical components. For the Netherlands, you'd want something like EPSG:7415 (RD New + NAP height).\n",
    "\n",
    "Have you used `pyproj` before, or would you like to explore how to set up this transformation? It will involve a few steps beyond the simple `.to_crs()` you're currently using."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c4722c",
   "metadata": {},
   "source": [
    "## Convert Pandas DataFrame to GeoDataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8829719b",
   "metadata": {},
   "source": [
    "How to convert the pandas dataframe `sl2df_sml` to a geodataframe? \n",
    "\n",
    "##### Reply<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "To convert your pandas DataFrame `sl2df_sml` to a GeoDataFrame, you need to create geometry objects from your coordinate columns. Since you have `longitude` and `latitude` columns, you can use those to create Point geometries.\n",
    "\n",
    "Here's how you'd approach it:\n",
    "\n",
    "```python\n",
    "gdf = geopandas.GeoDataFrame(sl2df_sml, geometry=geopandas.points_from_xy(sl2df_sml.longitude, sl2df_sml.latitude))\n",
    "```\n",
    "\n",
    "This creates Point geometries from your longitude/latitude columns. However, you'll also want to set the coordinate reference system (CRS) since these are WGS84 coordinates. Would you like to try writing that part yourself, or would you like a hint about which method to use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f7730d",
   "metadata": {
    "time_run": "3:52:52p"
   },
   "outputs": [],
   "source": [
    "gdf = gpd.GeoDataFrame(sl3_df_sml, geometry=gpd.points_from_xy(sl3_df_sml.longitude, sl3_df_sml.latitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff3b901",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gdf.set_crs(epsg=4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29aff19",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42010a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gdf.to_crs(epsg=28992)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89782c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.crs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe6456e",
   "metadata": {},
   "source": [
    "::: {.callout-tip collapse=\"true\"}\n",
    "##  AI regex chat\n",
    "I want to build a regex to extract the height in all of the following cases:\n",
    "\n",
    "`measurements_-720cmnap.sl2`\n",
    "`measurements_+720cmNap.sl3`\n",
    "`measurements_-1720cmnap.sl3`\n",
    "`measurements-1720cmnap.sl3`\n",
    "`measurements+20cmNAP.sl2`\n",
    "`measurements+20CMNAP.sl2`\n",
    "\n",
    "##### Reply<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Great! You want to extract the height value (including the sign) from those filenames. Let's break down what patterns you need to match:\n",
    "\n",
    "- Either a leading `+` or `-` sign\n",
    "- One or more digits\n",
    "- Followed by `cmnap` (case-insensitive)\n",
    "\n",
    "You only need the number with its sign.\n",
    "\n",
    "To extract this pattern with regex, you'll need to use Python's `re` module. Here's the approach:\n",
    "\n",
    "**Pattern components:**\n",
    "- `[+-]` matches either + or -\n",
    "- `\\d+` matches one or more digits\n",
    "- `(?=cmnap)` is a lookahead that checks for \"cmnap\" (case-insensitive) without including it in the match\n",
    "\n",
    "You'll want to use `re.search()` to find the pattern in the filename string, and use the `re.IGNORECASE` flag to handle different capitalizations.\n",
    "\n",
    "Would you like to try writing the regex pattern yourself, or would you like me to show you an example?\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972fd780",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgx = r\"[+-]\\d+(?=cmnap)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609c291c",
   "metadata": {},
   "outputs": [],
   "source": [
    "int(re.search(rgx, str(sl2_f), flags=re.IGNORECASE)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46610e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def extract_height(\n",
    "    sl_filepath: Path, # The absolute location of the file to convert\n",
    "    re_ptrn: str=r\"[+-]?\\d+(?=cmnap)\"\n",
    "    )->int: # The height of the measurement station in cm above NAP\n",
    "    \"Extract height from the filename in cmNAP\"\n",
    "    if 'cmnap' not in str(sl_filepath).lower():\n",
    "        raise ValueError(\"The filename must contain the height of the Sonar boot at time of measurement in 'cmNAP' at the end of the filename (e.g. 'example_description_+1050cmNAP.sl2')\")\n",
    "    return int(re.search(re_ptrn, str(sl_filepath), flags=re.IGNORECASE)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d235b3",
   "metadata": {},
   "source": [
    "Test function `extract_height`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f1c79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(extract_height(Path(\"/some/where/afen22e34_1823cmNAP.sl2\")),\n",
    "extract_height(Path(\"/some/where/afen22e34-1823cmNAP.sl2\")),\n",
    "extract_height(Path(\"/some/where/afen22e34br_-1823cmNAP.sl2\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d571d0",
   "metadata": {
    "time_run": "3:28:30p"
   },
   "outputs": [],
   "source": [
    "def slx2gdf(\n",
    "    sl_filepath: Path, # The absolute location of the file to convert\n",
    "    msrmnt_height: int, # Height of the measurement instrument at time of taking the measurements\n",
    "    to_crs: str = \"epsg:28992\", # epsg code of crs to transform the co繹rdinates to\n",
    "    )->gpd:\n",
    "    \"Convert a sl2 or sl3 file to a GeoDataFrame with the given crs.\"\n",
    "    s = Sonar(str(sl_filepath))\n",
    "    df = s.df\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.longitude, df.latitude))\n",
    "    gdf = gdf.set_crs(epsg=4326)\n",
    "    return gdf.to_crs(to_crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d44e4f",
   "metadata": {},
   "source": [
    "Test function `slx2gdf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b384e0",
   "metadata": {
    "time_run": "3:28:37p"
   },
   "outputs": [],
   "source": [
    "sl3_gdf = slx2gdf(sl3_f, 1823)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e56ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"water_depth\" in gdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae6fdff",
   "metadata": {
    "time_run": "3:29:12p"
   },
   "outputs": [],
   "source": [
    "sl3_gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89e66ac",
   "metadata": {},
   "source": [
    "## Building filter on GeoDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319fa4fd",
   "metadata": {
    "time_run": "3:56:32p"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>survey</th>\n",
       "      <th>datetime</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>min_range</th>\n",
       "      <th>max_range</th>\n",
       "      <th>water_depth</th>\n",
       "      <th>gps_speed</th>\n",
       "      <th>gps_heading</th>\n",
       "      <th>gps_altitude</th>\n",
       "      <th>bottom_index</th>\n",
       "      <th>frames</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1570</th>\n",
       "      <td>174</td>\n",
       "      <td>primary</td>\n",
       "      <td>2022-04-26 11:08:49.101999998</td>\n",
       "      <td>674208</td>\n",
       "      <td>6913399</td>\n",
       "      <td>6.076888</td>\n",
       "      <td>52.748741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.57600</td>\n",
       "      <td>0.606861</td>\n",
       "      <td>0.127106</td>\n",
       "      <td>0.246756</td>\n",
       "      <td>-1.91</td>\n",
       "      <td>50</td>\n",
       "      <td>[137, 137, 137, 137, 137, 129, 124, 119, 114, ...</td>\n",
       "      <td>POINT (201568.299 529266.871)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1579</th>\n",
       "      <td>175</td>\n",
       "      <td>primary</td>\n",
       "      <td>2022-04-26 11:08:49.249000072</td>\n",
       "      <td>674208</td>\n",
       "      <td>6913399</td>\n",
       "      <td>6.076888</td>\n",
       "      <td>52.748741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.99288</td>\n",
       "      <td>0.609836</td>\n",
       "      <td>0.122237</td>\n",
       "      <td>0.247439</td>\n",
       "      <td>-1.91</td>\n",
       "      <td>469</td>\n",
       "      <td>[216, 216, 216, 216, 216, 216, 216, 216, 216, ...</td>\n",
       "      <td>POINT (201568.299 529266.871)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1587</th>\n",
       "      <td>176</td>\n",
       "      <td>primary</td>\n",
       "      <td>2022-04-26 11:08:49.290999889</td>\n",
       "      <td>674208</td>\n",
       "      <td>6913399</td>\n",
       "      <td>6.076888</td>\n",
       "      <td>52.748741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.99288</td>\n",
       "      <td>0.609836</td>\n",
       "      <td>0.118954</td>\n",
       "      <td>0.248092</td>\n",
       "      <td>-1.91</td>\n",
       "      <td>469</td>\n",
       "      <td>[216, 216, 216, 216, 216, 216, 216, 216, 216, ...</td>\n",
       "      <td>POINT (201568.299 529266.871)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>177</td>\n",
       "      <td>primary</td>\n",
       "      <td>2022-04-26 11:08:49.378000021</td>\n",
       "      <td>674208</td>\n",
       "      <td>6913399</td>\n",
       "      <td>6.076888</td>\n",
       "      <td>52.748741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.99288</td>\n",
       "      <td>0.612811</td>\n",
       "      <td>0.114456</td>\n",
       "      <td>0.248715</td>\n",
       "      <td>-1.91</td>\n",
       "      <td>471</td>\n",
       "      <td>[216, 216, 216, 216, 216, 216, 216, 216, 216, ...</td>\n",
       "      <td>POINT (201568.299 529266.871)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1604</th>\n",
       "      <td>178</td>\n",
       "      <td>primary</td>\n",
       "      <td>2022-04-26 11:08:49.423000097</td>\n",
       "      <td>674208</td>\n",
       "      <td>6913399</td>\n",
       "      <td>6.076888</td>\n",
       "      <td>52.748741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.99288</td>\n",
       "      <td>0.612811</td>\n",
       "      <td>0.107427</td>\n",
       "      <td>0.249878</td>\n",
       "      <td>-1.91</td>\n",
       "      <td>471</td>\n",
       "      <td>[216, 216, 216, 216, 216, 216, 216, 216, 216, ...</td>\n",
       "      <td>POINT (201568.299 529266.871)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id   survey                      datetime       x        y  longitude  \\\n",
       "1570  174  primary 2022-04-26 11:08:49.101999998  674208  6913399   6.076888   \n",
       "1579  175  primary 2022-04-26 11:08:49.249000072  674208  6913399   6.076888   \n",
       "1587  176  primary 2022-04-26 11:08:49.290999889  674208  6913399   6.076888   \n",
       "1596  177  primary 2022-04-26 11:08:49.378000021  674208  6913399   6.076888   \n",
       "1604  178  primary 2022-04-26 11:08:49.423000097  674208  6913399   6.076888   \n",
       "\n",
       "       latitude  min_range  max_range  water_depth  gps_speed  gps_heading  \\\n",
       "1570  52.748741        0.0   36.57600     0.606861   0.127106     0.246756   \n",
       "1579  52.748741        0.0    3.99288     0.609836   0.122237     0.247439   \n",
       "1587  52.748741        0.0    3.99288     0.609836   0.118954     0.248092   \n",
       "1596  52.748741        0.0    3.99288     0.612811   0.114456     0.248715   \n",
       "1604  52.748741        0.0    3.99288     0.612811   0.107427     0.249878   \n",
       "\n",
       "      gps_altitude  bottom_index  \\\n",
       "1570         -1.91            50   \n",
       "1579         -1.91           469   \n",
       "1587         -1.91           469   \n",
       "1596         -1.91           471   \n",
       "1604         -1.91           471   \n",
       "\n",
       "                                                 frames  \\\n",
       "1570  [137, 137, 137, 137, 137, 129, 124, 119, 114, ...   \n",
       "1579  [216, 216, 216, 216, 216, 216, 216, 216, 216, ...   \n",
       "1587  [216, 216, 216, 216, 216, 216, 216, 216, 216, ...   \n",
       "1596  [216, 216, 216, 216, 216, 216, 216, 216, 216, ...   \n",
       "1604  [216, 216, 216, 216, 216, 216, 216, 216, 216, ...   \n",
       "\n",
       "                           geometry  \n",
       "1570  POINT (201568.299 529266.871)  \n",
       "1579  POINT (201568.299 529266.871)  \n",
       "1587  POINT (201568.299 529266.871)  \n",
       "1596  POINT (201568.299 529266.871)  \n",
       "1604  POINT (201568.299 529266.871)  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sl3_gdf_pr = sl3_gdf[sl3_gdf[\"survey\"]==\"primary\"]\n",
    "sl3_gdf_pr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342e2388",
   "metadata": {},
   "source": [
    "::: {.callout-tip collapse=\"true\"}\n",
    "Please explain the DataFrame warning.\n",
    "\n",
    "```python\n",
    "sl2_gdf_pr['bottom_height'] = 12.53/100 - sl2_gdf_pr['water_depth']\n",
    "```\n",
    "\n",
    "```text\n",
    "/app/data/.local/lib/python3.12/site-packages/geopandas/geodataframe.py:1968: SettingWithCopyWarning: \n",
    "A value is trying to be set on a copy of a slice from a DataFrame.\n",
    "Try using .loc[row_indexer,col_indexer] = value instead\n",
    "\n",
    "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
    "  super().__setitem__(key, value)\n",
    "```\n",
    "\n",
    "##### Reply<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "This warning occurs because of how you created `sl2df_sml_pr`. When you filtered the dataframe with:\n",
    "\n",
    "```python\n",
    "sl2df_sml_pr = sl2df_sml[sl2df_sml[\"survey\"]==\"primary\"]\n",
    "```\n",
    "\n",
    "Pandas doesn't know if this created a **copy** of the data or just a **view** (reference) to the original data. When you then try to add a new column to `sl2df_sml_pr`, pandas warns you that changes might unexpectedly affect the original `sl2df_sml` dataframe (or might not work at all).\n",
    "\n",
    "**The solution:** Use `.copy()` to explicitly create a new dataframe:\n",
    "\n",
    "```python\n",
    "sl2df_sml_pr = sl2df_sml[sl2df_sml[\"survey\"]==\"primary\"].copy()\n",
    "```\n",
    "\n",
    "Or use `.loc[]` for the assignment as the warning suggests. \n",
    "\n",
    "Would you like to try fixing this yourself, or do you have questions about why this happens?\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e7d1d6",
   "metadata": {
    "time_run": "3:33:25p"
   },
   "outputs": [],
   "source": [
    "sl3_gdf_pr = sl3_gdf[sl3_gdf[\"survey\"]==\"primary\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b293e2",
   "metadata": {
    "time_run": "3:33:52p"
   },
   "outputs": [],
   "source": [
    "sl3_gdf_pr['bottom_height'] = 12.53/100 - sl3_gdf_pr['water_depth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c7f8d6",
   "metadata": {
    "time_run": "3:34:12p"
   },
   "outputs": [],
   "source": [
    "sl3_gdf_pr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2178b81e",
   "metadata": {},
   "source": [
    "::: {.callout-tip collapse=\"true\"}\n",
    "## AI chat: `latitude` and `longitude`\n",
    "\n",
    " _How can I check the precision of the `latitude` and `longitude` columns?_\n",
    "\n",
    "##### Reply<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "To check the precision of the `latitude` and `longitude` columns, you can look at how many decimal places the values have. Here are a few approaches:\n",
    "\n",
    "**Option 1:** Convert to string and check the length after the decimal point:\n",
    "```python\n",
    "sl2_gdf_pr['latitude'].astype(str).str.split('.').str[1].str.len()\n",
    "```\n",
    "\n",
    "**Option 2:** Calculate the smallest difference between consecutive values to see the resolution.\n",
    "\n",
    "**Option 3:** Look at a few sample values directly to visually inspect the decimal places.\n",
    "\n",
    "Which approach would you like to try first, or do you have a specific reason for checking the precision?\n",
    "\n",
    "---\n",
    " _\"Do I understand correctly that the visualisation from a Dataframe shows the complete numbers and doesn't truncate?\"_\n",
    "\n",
    "##### Reply<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Not quite - pandas DataFrames **do truncate** displayed values by default for readability, even though the full precision is stored internally.\n",
    "\n",
    "You can check the actual precision by:\n",
    "\n",
    "1. Using `.iloc[0]` to access a single value directly (this shows more precision than the table view)\n",
    "2. Checking pandas display options with `pd.get_option('display.precision')`\n",
    "3. Setting display precision higher with `pd.set_option('display.precision', 10)` (or whatever number you want)\n",
    "\n",
    "Try accessing a single latitude value directly - does it show more decimal places than what you see in the table display?Not quite - pandas DataFrames **do** truncate displayed values by default! When you see a value like `52.71157` in the display, there might actually be more decimal places that aren't being shown.\n",
    "\n",
    "To see the full precision of the values, you can:\n",
    "\n",
    "1. Use `pd.set_option('display.precision', 20)` to show more decimal places in the display\n",
    "2. Access individual values directly with `.iloc[]` or `.loc[]` and print them\n",
    "3. Check the actual dtype of the column - if it's `float64`, it can store about 15-17 significant decimal digits\n",
    "\n",
    "Would you like to try one of these methods to see the full precision of your latitude/longitude values?\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574209dc",
   "metadata": {
    "time_run": "3:49:45p"
   },
   "outputs": [],
   "source": [
    "sl3_gdf_pr['longitude'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a846507",
   "metadata": {},
   "source": [
    "::: {.callout-tip collapse=\"true\"}\n",
    "##  AI Chat: Geometry column\n",
    "\n",
    " _How can I check if the precision of the `geometry` column is the same?_\n",
    "\n",
    "##### Reply<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Good question! The `geometry` column contains Point objects (not just numbers), so you need to extract the coordinates from the geometry first.\n",
    "\n",
    "You can access the x and y coordinates of a Point geometry using `.x` and `.y` attributes:\n",
    "\n",
    "```python\n",
    "sl2_gdf_pr['geometry'].iloc[0].x\n",
    "sl2_gdf_pr['geometry'].iloc[0].y\n",
    "```\n",
    "\n",
    "This will show you the precision of the transformed coordinates (in RD New/EPSG:28992). Try it and compare - do they have similar precision to the original longitude/latitude values?\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697ee39e",
   "metadata": {
    "time_run": "3:50:15p"
   },
   "outputs": [],
   "source": [
    "sl3_gdf_pr['geometry'].iloc[0].x\n",
    "sl3_gdf_pr['geometry'].iloc[0].y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ceb0400",
   "metadata": {},
   "source": [
    "::: {.callout-tip collapse=\"true\"}\n",
    "##  AI chat: Check meaning of `bottom_index` column\n",
    "\n",
    " _I think that all measurements with the same \"bottom_index\" also have the same longitude and latitude and water_depth. How can I check that assumption?_\n",
    "\n",
    "##### Reply<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Good hypothesis! To check this, you could group by `bottom_index` and see if the values within each group are all the same.\n",
    "\n",
    "One approach is to use `.groupby()` with `.nunique()` (number of unique values). If a column has only 1 unique value per group, then all values in that group are identical.\n",
    "\n",
    "Try something like:\n",
    "\n",
    "```python\n",
    "sl2df_sml_pr.groupby('bottom_index')[['longitude', 'latitude', 'water_depth']].nunique()\n",
    "```\n",
    "\n",
    "This will show you how many unique values exist for each column within each `bottom_index` group. What do you expect to see if your assumption is correct?\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686dcba0",
   "metadata": {
    "time_run": "3:57:47p"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>water_depth</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bottom_index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>322 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              longitude  latitude  water_depth\n",
       "bottom_index                                  \n",
       "50                    1         1            1\n",
       "328                   1         2            1\n",
       "330                   2         2            1\n",
       "332                   1         1            1\n",
       "333                   2         2            2\n",
       "...                 ...       ...          ...\n",
       "1001                  1         1            1\n",
       "1005                  2         1            1\n",
       "1007                  1         1            1\n",
       "1008                  3         2            2\n",
       "1011                  1         2            1\n",
       "\n",
       "[322 rows x 3 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sl3_gdf[sl3_gdf[\"survey\"]==\"primary\"].groupby('bottom_index')[['longitude', 'latitude', 'water_depth']].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc61d9c",
   "metadata": {
    "time_run": "3:58:13p"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>survey</th>\n",
       "      <th>datetime</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>min_range</th>\n",
       "      <th>max_range</th>\n",
       "      <th>water_depth</th>\n",
       "      <th>gps_speed</th>\n",
       "      <th>gps_heading</th>\n",
       "      <th>gps_altitude</th>\n",
       "      <th>bottom_index</th>\n",
       "      <th>frames</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9536</th>\n",
       "      <td>1111</td>\n",
       "      <td>primary</td>\n",
       "      <td>2022-04-26 11:09:52.848999977</td>\n",
       "      <td>674198</td>\n",
       "      <td>6913407</td>\n",
       "      <td>6.076798</td>\n",
       "      <td>52.748784</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.99288</td>\n",
       "      <td>0.617507</td>\n",
       "      <td>0.162986</td>\n",
       "      <td>4.224975</td>\n",
       "      <td>-5.5</td>\n",
       "      <td>475</td>\n",
       "      <td>[214, 214, 214, 214, 214, 214, 214, 214, 214, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9539</th>\n",
       "      <td>1111</td>\n",
       "      <td>secondary</td>\n",
       "      <td>2022-04-26 11:09:52.848999977</td>\n",
       "      <td>674198</td>\n",
       "      <td>6913407</td>\n",
       "      <td>6.076798</td>\n",
       "      <td>52.748784</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.99288</td>\n",
       "      <td>0.617507</td>\n",
       "      <td>0.162986</td>\n",
       "      <td>4.224975</td>\n",
       "      <td>-5.5</td>\n",
       "      <td>475</td>\n",
       "      <td>[214, 214, 214, 214, 214, 214, 214, 214, 214, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9544</th>\n",
       "      <td>1112</td>\n",
       "      <td>primary</td>\n",
       "      <td>2022-04-26 11:09:52.894999981</td>\n",
       "      <td>674198</td>\n",
       "      <td>6913407</td>\n",
       "      <td>6.076798</td>\n",
       "      <td>52.748784</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.99288</td>\n",
       "      <td>0.617507</td>\n",
       "      <td>0.164213</td>\n",
       "      <td>4.223330</td>\n",
       "      <td>-5.5</td>\n",
       "      <td>475</td>\n",
       "      <td>[214, 214, 214, 214, 214, 214, 214, 214, 214, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9547</th>\n",
       "      <td>1112</td>\n",
       "      <td>secondary</td>\n",
       "      <td>2022-04-26 11:09:52.894999981</td>\n",
       "      <td>674198</td>\n",
       "      <td>6913407</td>\n",
       "      <td>6.076798</td>\n",
       "      <td>52.748784</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.99288</td>\n",
       "      <td>0.617507</td>\n",
       "      <td>0.164213</td>\n",
       "      <td>4.223330</td>\n",
       "      <td>-5.5</td>\n",
       "      <td>475</td>\n",
       "      <td>[214, 214, 214, 214, 214, 214, 214, 214, 214, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id     survey                      datetime       x        y  \\\n",
       "9536  1111    primary 2022-04-26 11:09:52.848999977  674198  6913407   \n",
       "9539  1111  secondary 2022-04-26 11:09:52.848999977  674198  6913407   \n",
       "9544  1112    primary 2022-04-26 11:09:52.894999981  674198  6913407   \n",
       "9547  1112  secondary 2022-04-26 11:09:52.894999981  674198  6913407   \n",
       "\n",
       "      longitude   latitude  min_range  max_range  water_depth  gps_speed  \\\n",
       "9536   6.076798  52.748784        0.0    3.99288     0.617507   0.162986   \n",
       "9539   6.076798  52.748784        0.0    3.99288     0.617507   0.162986   \n",
       "9544   6.076798  52.748784        0.0    3.99288     0.617507   0.164213   \n",
       "9547   6.076798  52.748784        0.0    3.99288     0.617507   0.164213   \n",
       "\n",
       "      gps_heading  gps_altitude  bottom_index  \\\n",
       "9536     4.224975          -5.5           475   \n",
       "9539     4.224975          -5.5           475   \n",
       "9544     4.223330          -5.5           475   \n",
       "9547     4.223330          -5.5           475   \n",
       "\n",
       "                                                 frames  \n",
       "9536  [214, 214, 214, 214, 214, 214, 214, 214, 214, ...  \n",
       "9539  [214, 214, 214, 214, 214, 214, 214, 214, 214, ...  \n",
       "9544  [214, 214, 214, 214, 214, 214, 214, 214, 214, ...  \n",
       "9547  [214, 214, 214, 214, 214, 214, 214, 214, 214, ...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sl3_d.df[sl3_d.df[\"bottom_index\"]==475]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f144fb03",
   "metadata": {},
   "source": [
    "::: {.callout-tip collapse=\"true\"}\n",
    "##  AI chat: Further investigate `bottom_index`\n",
    "\n",
    " _It is mostly true that the same bottom_index is the same location and depth. But not always. So we must filter the resulting table on same locations. Could we use something like `.unique` to only keep those points that have a unique `longitude`, `latitude` combination?_\n",
    "\n",
    "##### Reply<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Yes, good thinking! You can use `.drop_duplicates()` to keep only unique combinations of longitude and latitude.\n",
    "\n",
    "The method would look like:\n",
    "\n",
    "```python\n",
    "df_unique = df.drop_duplicates(subset=['longitude', 'latitude'])\n",
    "```\n",
    "\n",
    "This keeps the first occurrence of each unique longitude/latitude pair and removes the rest.\n",
    "\n",
    "Would you like to try applying this to your filtered primary survey data? Or do you have questions about how `drop_duplicates()` works?\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9647a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_gdf(\n",
    "    gdf: gpd.GeoDataFrame, # GeoDataFrame from sl2gdf\n",
    "    msrmnt_height: int, # Height of measurement instrument in cm above NAP\n",
    "    ) -> gpd.GeoDataFrame: # Cleaned GeoDataFrame with bottom_height column\n",
    "    \"Filter primary survey data, remove duplicates, and calculate bottom height in mNAP\"\n",
    "    gdf_primary = gdf[gdf[\"survey\"] == \"primary\"].copy()\n",
    "    gdf_unique = gdf_primary.drop_duplicates(subset=['longitude', 'latitude', 'water_depth'])\n",
    "    gdf_unique.loc[:, 'bottom_height'] = msrmnt_height / 100 - gdf_unique['water_depth']\n",
    "    return gdf_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6adfaa",
   "metadata": {
    "time_run": "4:40:43p"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clean_gdf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m",
      "\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m cln_gdf = \u001b[43mclean_gdf\u001b[49m(sl2_gdf, \u001b[32m1823\u001b[39m)",
      "",
      "\u001b[31mNameError\u001b[39m: name 'clean_gdf' is not defined"
     ]
    }
   ],
   "source": [
    "cln_gdf = clean_gdf(sl2_gdf, 1823)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e5bd44",
   "metadata": {},
   "source": [
    "Why do I get this warning in the function `clean_gdf`?\n",
    "\n",
    "##### Reply<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "The warning occurs because `.drop_duplicates()` might return a view rather than a copy, and then you're trying to modify it.\n",
    "\n",
    "The fix is to add `.copy()` after `.drop_duplicates()`:\n",
    "\n",
    "```python\n",
    "gdf_unique = gdf_primary.drop_duplicates(subset=['longitude', 'latitude', 'water_depth']).copy()\n",
    "```\n",
    "\n",
    "This ensures `gdf_unique` is definitely a new dataframe, not a view. Would you like to update the function with this change?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14659d2",
   "metadata": {},
   "source": [
    "Is this really the best way? We now create a bunch of copies in memory, just to clean the dataframe.\n",
    "\n",
    "##### Reply<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "You're absolutely right to question this! Creating multiple copies is inefficient, especially with large datasets.\n",
    "\n",
    "A better approach would be to chain the operations and only make one copy at the end:\n",
    "\n",
    "```python\n",
    "gdf_clean = (gdf[gdf[\"survey\"] == \"primary\"]\n",
    "             .drop_duplicates(subset=['longitude', 'latitude', 'water_depth'])\n",
    "             .copy())\n",
    "gdf_clean['bottom_height'] = msrmnt_height / 100 - gdf_clean['water_depth']\n",
    "```\n",
    "\n",
    "Or even more efficiently, you could use `.assign()` to add the column without triggering the warning:\n",
    "\n",
    "```python\n",
    "return (gdf[gdf[\"survey\"] == \"primary\"]\n",
    "        .drop_duplicates(subset=['longitude', 'latitude', 'water_depth'])\n",
    "        .assign(bottom_height=lambda x: msrmnt_height / 100 - x['water_depth']))\n",
    "```\n",
    "\n",
    "This creates only one copy and avoids the warning entirely. Would you like to try refactoring your function with one of these approaches?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42f4716",
   "metadata": {},
   "source": [
    "There also is another issue. Sometimes we do have the same location (e.g. 'longitude', 'latitude') but different heights. This is probably due to the fact that the depth meter creates a pulse almost every second, but the location is rounded off to respectively 6 and 5 numbers after the diggit. So I actually want to drop the long-lat duplicates and take the mean of the merged 'water_depth'.\n",
    "\n",
    "##### Reply<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Good observation! So you want to group by longitude and latitude, then take the mean of `water_depth` for each unique location.\n",
    "\n",
    "You can use `.groupby()` with `.agg()` to accomplish this. The tricky part is handling the geometry column and deciding what to do with the other columns (like datetime, gps_speed, etc.).\n",
    "\n",
    "For the geometry, you could keep the first point since they're all at the same location. For other columns, you'll need to decide: keep first? mean? something else?\n",
    "\n",
    "Would you like to try writing the groupby operation yourself, or would you like me to show you an example of how to structure it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454a19f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc086c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_grpd = gdf.groupby(['longitude', 'latitude'], as_index=False).agg(\r\n",
    "    mean_depth=pd.NamedAgg(column=\"water_depth\", aggfunc=\"mean\"),\r\n",
    "    min_depth=pd.NamedAgg(column=\"water_depth\", aggfunc=\"min\"),\r\n",
    "    max_depth=pd.NamedAgg(column=\"water_depth\", aggfunc=\"max\"),\r\n",
    "    geometery=pd.NamedAgg(column=\"geometry\", aggfunc=\"first\"),\r\n",
    "    datetime=pd.NamedAgg(column=\"datetime\", aggfunc=\"mean\")\r\n",
    ")\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728ac084",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_grpd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a42097b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_gdf(\n",
    "    gdf: gpd.GeoDataFrame, # GeoDataFrame from sl2gdf\n",
    "    msrmnt_height: int, # Height of measurement instrument in cm above NAP\n",
    "    ) -> gpd.GeoDataFrame: # Cleaned GeoDataFrame with bottom_height column\n",
    "    \"Filter primary survey data, remove duplicates, and calculate bottom height in mNAP\"\n",
    "    return (gdf[gdf[\"survey\"]==\"primary\"]\n",
    "            .drop_duplicates(subset=['longitude', 'latitude', 'water_depth'])\n",
    "            .assign(bottom_height=lambda x: msrmnt_height / 100 - x['water_depth']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d16030d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_gdf(\n",
    "    gdf: gpd.GeoDataFrame, # GeoDataFrame from sl2gdf\n",
    "    msrmnt_height: int, # Height of measurement instrument in cm above NAP\n",
    "    ) -> gpd.GeoDataFrame: # Cleaned GeoDataFrame with bottom_height column\n",
    "    \"Filter primary survey data, remove duplicates, and calculate bottom height in mNAP\"\n",
    "    return (gdf[gdf[\"survey\"]==\"primary\"]\n",
    "            .groupby(['longitude', 'latitude'], as_index=False).agg(\n",
    "                mean_depth=pd.NamedAgg(column=\"water_depth\", aggfunc=\"mean\"),\n",
    "                min_depth=pd.NamedAgg(column=\"water_depth\", aggfunc=\"min\"),\n",
    "                max_depth=pd.NamedAgg(column=\"water_depth\", aggfunc=\"max\"),\n",
    "                geometry=pd.NamedAgg(column=\"geometry\", aggfunc=\"first\"),\n",
    "                datetime=pd.NamedAgg(column=\"datetime\", aggfunc=\"mean\")\n",
    "            )\n",
    "            .assign(bottom_height=lambda x: msrmnt_height / 100 - x['mean_depth']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d5bd06",
   "metadata": {},
   "source": [
    "When I use `clean_gdf` the returned object is a Pandas Dataframe, not a GeoDataFrame. This probably is the result of the `.groupby` method that uses the `agg` method. How can I solve this? I want a GeoDataFrame?\n",
    "\n",
    "##### Reply<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Good catch! When you use `.groupby().agg()` on a GeoDataFrame, it returns a regular DataFrame, losing the GeoDataFrame properties.\n",
    "\n",
    "To fix this, you need to convert it back to a GeoDataFrame after the aggregation. You can use:\n",
    "\n",
    "```python\n",
    "gpd.GeoDataFrame(result, geometry='geometry', crs=gdf.crs)\n",
    "```\n",
    "\n",
    "Where `result` is your aggregated dataframe. You'll need to make sure to preserve the original CRS from the input GeoDataFrame.\n",
    "\n",
    "Would you like to try adding this conversion to your `clean_gdf` function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e87f446",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def clean_gdf(\n",
    "    gdf: gpd.GeoDataFrame, # GeoDataFrame from sl2gdf\n",
    "    msrmnt_height: int, # Height of measurement instrument in cm above NAP\n",
    "    ) -> gpd.GeoDataFrame: # Cleaned GeoDataFrame with bottom_height column\n",
    "    \"Filter primary survey data, remove duplicates, and calculate bottom height in mNAP\"\n",
    "    df_cln = (gdf[gdf[\"survey\"]==\"primary\"]\n",
    "            .groupby(['longitude', 'latitude'], as_index=False).agg(\n",
    "                mean_depth=pd.NamedAgg(column=\"water_depth\", aggfunc=\"mean\"),\n",
    "                min_depth=pd.NamedAgg(column=\"water_depth\", aggfunc=\"min\"),\n",
    "                max_depth=pd.NamedAgg(column=\"water_depth\", aggfunc=\"max\"),\n",
    "                geometry=pd.NamedAgg(column=\"geometry\", aggfunc=\"first\"),\n",
    "                datetime=pd.NamedAgg(column=\"datetime\", aggfunc=\"mean\")\n",
    "            )\n",
    "            .assign(bottom_height=lambda x: msrmnt_height / 100 - x['mean_depth']))\n",
    "    return gpd.GeoDataFrame(df_cln, geometry='geometry', crs=gdf.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55583aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cln_gdf = clean_gdf(sl2_gdf, 1823)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8507fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(cln_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2d9c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "cln_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bd9f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "cln_gdf['geometry'].iloc[0].x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bd859c",
   "metadata": {},
   "source": [
    "I also would like to check if the Dataframe that is created from the sl2 or sl2 file contains the needed columns in the needed datatype. Should I write a function to check that? Should I add the checks within an existing function? Should I use Pydantic?\n",
    "\n",
    "##### Reply<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Good question about validation! You have a few options:\n",
    "\n",
    "1. **Separate validation function** - keeps concerns separated, reusable\n",
    "2. **Add checks within existing functions** - catches issues early where they matter\n",
    "3. **Use Pydantic** - more robust, but adds dependency and complexity\n",
    "\n",
    "For your case, I'd suggest starting simple: add basic checks within your existing functions (like `sl2gdf` or `clean_gdf`) to verify the required columns exist. Something like:\n",
    "\n",
    "```python\n",
    "required_cols = ['longitude', 'latitude', 'water_depth', 'survey']\n",
    "if not all(col in df.columns for col in required_cols):\n",
    "    raise ValueError(f\"Missing required columns...\")\n",
    "```\n",
    "\n",
    "Pydantic would be great if you're building a larger system with complex validation needs, but might be overkill here.\n",
    "\n",
    "Which approach feels right for your use case? Are you expecting the sonar file format to be stable, or do you need robust validation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a4c698",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def slx2gdf(\r\n",
    "    sl_filepath: Path, # The absolute location of the file to convert\r\n",
    "    to_crs: str = \"epsg:28992\", # epsg code of crs to transform the co繹rdinates to\r\n",
    "    survey_fltr: str = \"primary\", # Filter measurement facts on survey value\r\n",
    "    )->gpd:\r\n",
    "    \"Convert a sl2 or sl3 file to a GeoDataFrame with the given crs.\"\r\n",
    "    s = Sonar(str(sl_filepath))\r\n",
    "    df = s.df\r\n",
    "    required_cols = ['longitude', 'latitude', 'water_depth', 'survey']\r\n",
    "    if not all(col in df.columns for col in required_cols):\r\n",
    "        raise KeyError(f\"Missing one or more of the required columns in the converted sl2 or sl3 file.\\nRequired columns are: 'longitude', 'latitude', 'water_depth' and 'survey'\")\r\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.longitude, df.latitude))\r\n",
    "    gdf = gdf.set_crs(epsg=4326)\r\n",
    "    return gdf.to_crs(to_crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c45be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = slx2gdf(sl2_f)\r\n",
    "gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425b120d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e9ba4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf['geometry'].iloc[0].x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ebeee4",
   "metadata": {},
   "source": [
    "I want to export a  GeoDataframe to different filetypes. See `export_gdf` function. How do I do that?\r\n",
    "\r\n",
    "def export_gdf(\r\n",
    "    gdf: gpd.GeoDataFrame, # GeoDataFrame to be saved\r\n",
    "    fn: str, # Filename of the GeoDataFrame without extension\r\n",
    "    folder_out: Path, # Absolute path to folder where files can be saved\r\n",
    "    esri_shp: bool=True, # Save GeoDataFrame to Esri shapefile?\r\n",
    "    csv: bool=True, # Save GeoDataFrame to comma seperated file?\r\n",
    "    geopckg: bool=True, # Save GeoDataFrame to geopackage?\r\n",
    "    )->None:\r\n",
    "    if esri_shp: pass\r\n",
    "    if csv: pass\r\n",
    "    if geopckg: pass\n",
    "\n",
    "##### Reply<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "To export a GeoDataFrame to different file formats, you can use geopandas' built-in methods. Here's how to implement each:\n",
    "\n",
    "**Shapefile:**\n",
    "```python\n",
    "gdf.to_file(folder_out / \"filename.shp\")\n",
    "```\n",
    "\n",
    "**CSV:**\n",
    "```python\n",
    "gdf.to_csv(folder_out / \"filename.csv\", index=False)\n",
    "```\n",
    "\n",
    "**GeoPackage:**\n",
    "```python\n",
    "gdf.to_file(folder_out / \"filename.gpkg\", driver=\"GPKG\")\n",
    "```\n",
    "\n",
    "For the CSV export, you might want to drop the geometry column first (since CSV can't store geometry) or convert it to WKT format. What would you prefer - include geometry as WKT text, or just export the coordinate columns without the geometry object?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc757b2",
   "metadata": {},
   "source": [
    "I don't want to drop the geometry column completely. I want to keep the x and y coordinates in a x and y column. How do I do that?\n",
    "\n",
    "##### Reply<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "You can extract the x and y coordinates from the geometry column before exporting to CSV. Here's how:\n",
    "\n",
    "```python\n",
    "gdf['x'] = gdf.geometry.x\n",
    "gdf['y'] = gdf.geometry.y\n",
    "```\n",
    "\n",
    "Then when you export to CSV, you can drop just the geometry column:\n",
    "\n",
    "```python\n",
    "gdf.drop(columns='geometry').to_csv(folder_out / f\"{fn}.csv\", index=False)\n",
    "```\n",
    "\n",
    "Would you like to add this to your `export_gdf` function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f137329d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def export_gdf(\n",
    "    gdf: gpd.GeoDataFrame, # GeoDataFrame to be saved\n",
    "    fn: str, # Filename of the GeoDataFrame without extension\n",
    "    folder_out: Path, # Absolute path to folder where files can be saved\n",
    "    esri_shp: bool=True, # Save GeoDataFrame to Esri shapefile?\n",
    "    csv: bool=True, # Save GeoDataFrame to comma seperated file?\n",
    "    geopckg: bool=True, # Save GeoDataFrame to geopackage?\n",
    "    )->None:\n",
    "    if esri_shp: gdf.to_file(folder_out / f\"{fn}.shp\")\n",
    "    if geopckg: gdf.to_file(folder_out / f\"{fn}.gpkg\", driver=\"GPKG\")\n",
    "    if csv:\n",
    "        gdf['x'] = gdf.geometry.x\n",
    "        gdf['y'] = gdf.geometry.y\n",
    "        df = gdf.drop(columns=['geometry'])\n",
    "        df.to_csv(folder_out / f\"{fn}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb1850b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sl2_f.stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e0dff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_gdf(cln_gdf, sl2_f.stem, Path(\"../test/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db301e9",
   "metadata": {},
   "source": [
    "Write a function that combines all the functions above. The function should accept a path to a sl2 or sl3 file and a path to store the resulting \"shape\", \"csv\" and \"geopackage\" files with the properly georeferenced GeoDataframe with the cleaned data and bottom_height column.\n",
    "\n",
    "##### Reply<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Here's how you can combine all the functions:\n",
    "\n",
    "```python\n",
    "def process_sonar_file(\n",
    "    sl_filepath: Path, # Path to sl2 or sl3 file\n",
    "    folder_out: Path, # Output folder for exported files\n",
    "    to_crs: str = \"epsg:28992\" # Target CRS\n",
    "    ) -> None:\n",
    "    \"Process sonar file: extract height, convert to GeoDataFrame, clean, and export\"\n",
    "    msrmnt_height = extract_height(sl_filepath)\n",
    "    gdf = slx2gdf(sl_filepath, to_crs=to_crs)\n",
    "    gdf_clean = clean_gdf(gdf, msrmnt_height)\n",
    "    export_gdf(gdf_clean, sl_filepath.stem, folder_out)\n",
    "```\n",
    "\n",
    "This chains together: extracting the height from the filename, converting to GeoDataFrame, cleaning the data, and exporting to all three formats.\n",
    "\n",
    "Does this capture what you need?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e8b5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def process_sonar_file(\n",
    "    sl_filepath: Path, # Path to sl2 or sl3 file\n",
    "    folder_out: Path, # Output folder for exported files\n",
    "    to_crs: str = \"epsg:28992\" # Target CRS\n",
    "    ) -> None:\n",
    "    \"Process sonar file: extract height, convert to GeoDataFrame, clean, and export\"\n",
    "    msrmnt_height = extract_height(sl_filepath)\n",
    "    gdf = slx2gdf(sl_filepath, to_crs=to_crs)\n",
    "    gdf_clean = clean_gdf(gdf, msrmnt_height)\n",
    "    export_gdf(gdf_clean, sl_filepath.stem, folder_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06113e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_sonar_file(sl2_f, Path(\"../test\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f74017",
   "metadata": {},
   "source": [
    "Can you explain what the difference is \"under the hood\" between `gdf.loc[:, 'y'] =` and just `gdf['y'] = `?\n",
    "\n",
    "##### Reply<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Great question! Here's what's happening under the hood:\n",
    "\n",
    "**`gdf['y'] = ...`** (chained indexing)\n",
    "- Pandas first evaluates `gdf['y']`, which *might* return a view or a copy (pandas can't always tell)\n",
    "- Then it tries to assign to that result\n",
    "- Because pandas isn't sure if you're modifying the original or a copy, it warns you\n",
    "\n",
    "**`gdf.loc[:, 'y'] = ...`** (explicit indexing)\n",
    "- This is a single operation that tells pandas: \"I want to modify the original DataFrame at all rows (`:`) and column `'y'`\"\n",
    "- Pandas knows you intend to modify the original, so no ambiguity, no warning\n",
    "\n",
    "The `.loc[]` approach is more explicit about your intent to modify the DataFrame in place, which helps pandas optimize the operation and avoid unnecessary copies.\n",
    "\n",
    "Does that clarify the difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c83ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "solveit_dialog_mode": "learning",
  "solveit_ver": 2
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
